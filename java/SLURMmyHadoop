#!/bin/bash

#SBATCH --partition=general-compute

#SBATCH --time=40:15:00

#SBATCH --nodes=2

#SBATCH --ntasks-per-node=12

##SBATCH --constraint=CPU-L5520

#SBATCH --exclusive

##SBATCH --mem=24000

##SBATCH --mem-per-cpu=4000

#SBATCH --job-name="s-2m"

#SBATCH --output=output_medium_2_12.txt

#SBATCH --mail-user=pn33@buffalo.edu

#SBATCH --mail-type=ALL

##SBATCH --requeue

#Specifies that the job will be requeued after a node failure.

#The default is that the job will not be requeued.

#

#This SLURM script is modified version of the SDSC script

# found in /util/myhadoop/myHadoop-0.30b/examples.

# CDC Nov 5, 2014

#

echo "SLURM_JOBID="$SLURM_JOBID

echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST

echo "SLURM_NNODES"=$SLURM_NNODES

echo "SLURMTMPDIR="$SLURMTMPDIR


echo "working directory = "$SLURM_SUBMIT_DIR

module load java/1.6.0_22

module load hadoop/2.5.1

module load myhadoop/0.30b

module list

echo "MH_HOME="$MH_HOME

echo "HADOOP_HOME="$HADOOP_HOME

echo "JAVA_HOME="$JAVA_HOME

echo "Setting HADOOP to use SLURMTMPDIR on the local disk"

export MH_SCRATCH_DIR=$SLURMTMPDIR

echo "MH_SCRATCH_DIR="$MH_SCRATCH_DIR

#### Set this to the directory where Hadoop configs should be generated

# Don't change the name of this variable (HADOOP_CONF_DIR) as it is

# required by Hadoop - all config files will be picked up from here

#

# Make sure that this is accessible to all nodes
#### prakash changing the config files
###export HADOOP_CONF_DIR=$SLURM_SUBMIT_DIR/config-$SLURM_JOBID

export HADOOP_CONF_DIR=/gpfs/scratch/pn33/config-$SLURM_JOBID


echo "MyHadoop config directory="$HADOOP_CONF_DIR

### Set up the configuration

# Make sure number of nodes is the same as what you have requested from PBS

# usage: $myhadoop-configure.sh -h

# this is the non-persistent mode

NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`

echo "-------Set up the configurations for myHadoop"

$MH_HOME/bin/myhadoop-configure.sh

echo "-------Start hdfs and yarn ---"

$HADOOP_HOME/sbin/start-all.sh

#### Format HDFS, if this is the first time or not a persistent instance

echo "-------Show Report ---"

$HADOOP_HOME/bin/hadoop dfsadmin -report

echo "-------make directory ---"

$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -mkdir /data

echo "-------copy file to hdfs ---"

export FILENAME=./data/AB_$1_$1_$1_$1

echo "-------The input file name is: "$1

$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -put $1/* /data/

echo "-------list directory ---"

$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /data

export HFILE=/data/

echo "-------do mm of file ---"

$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar mapreduce.jar Main $HFILE myoutput-$SLURM_JOBID

echo "-------list output ---"

$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls myoutput-$SLURM_JOBID

echo "-------Get outout ---"

$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -get myoutput-$SLURM_JOBID $2 


echo "-------Stop hdfs and yarn ---"

$HADOOP_HOME/sbin/stop-all.sh


#### Clean up the working directories after job completion

$MH_HOME/bin/myhadoop-cleanup.sh