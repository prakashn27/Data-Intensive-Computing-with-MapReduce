#!/bin/bash
#SBATCH --partition=general-compute
#SBATCH --time=00:15:00
#SBATCH --nodes=3
#SBATCH --ntasks-per-node=12
#SBATCH --exclusive
##SBATCH --mem=24000
##SBATCH --mem-per-cpu=4000
#SBATCH --job-name="test_hbase"
#SBATCH --output=test-%J.out
#SBATCH --mail-user=ruhansa@buffalo.edu
##SBATCH --mail-type=ALL
##SBATCH --requeue
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.
#
#This SLURM script is modified version of the SDSC script
# found in /util/academic/myhadoop/myHadoop-0.30b/examples.
# CDC January 29, 2015
#
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR

echo "working directory = "$SLURM_SUBMIT_DIR

module load java/1.6.0_22
module load hadoop/2.5.1
module load hbase/0.98.10.1
module load myhadoop/0.30b
module list
echo "MH_HOME="$MH_HOME
echo "HADOOP_HOME="$HADOOP_HOME
echo "Setting HADOOP to use SLURMTMPDIR on the local disk"
export MH_SCRATCH_DIR=$SLURMTMPDIR
echo "MH_SCRATCH_DIR="$MH_SCRATCH_DIR
#### Set this to the directory where Hadoop configs should be generated
# Don't change the name of this variable (HADOOP_CONF_DIR) as it is
# required by Hadoop - all config files will be picked up from here
#
# Make sure that this is accessible to all nodes
export HADOOP_CONF_DIR=$SLURM_SUBMIT_DIR/config-$SLURM_JOBID
export HBASE_CONF_DIR=$SLURM_SUBMIT_DIR/config-$SLURM_JOBID
echo "HBASE_HOME="$HBASE_HOME
echo "HBASE_CONF_DIR="$HBASE_CONF_DIR
mkdir hbase-logs-$SLURM_JOBID
export MY_LOG_DIR=$SLURM_SUBMIT_DIR/hbase-logs-$SLURM_JOBID
### Set up the configuration
# Make sure number of nodes is the same as what you have requested from PBS
# usage: $myhadoop-configure.sh -h
# this is the non-persistent mode
NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo "-------Set up the configurations for myHadoop"
$MH_HOME/bin/myhadoop-configure.sh 
#
#
cp $HBASE_HOME/conf/hbase-env.sh-sample $HBASE_CONF_DIR/hbase-env.sh
cp $HBASE_HOME/conf/hbase-site.xml-sample $HBASE_CONF_DIR/hbase-site.xml
cp $HADOOP_CONF_DIR/slaves $HBASE_CONF_DIR/regionservers
#
# Note:  You should have at least three compute nodes for fully distributed.
#http://hbase.apache.org/book.html#_getting_started
echo "You should have at least three compute nodes for fully distributed."
NODE_A=`cat  $HADOOP_CONF_DIR/slaves | awk 'NR==1{print;exit}'`
NODE_B=`cat  $HADOOP_CONF_DIR/slaves | awk 'NR==2{print;exit}'`
NODE_C=`cat  $HADOOP_CONF_DIR/slaves | awk 'NR==3{print;exit}'`
echo "Node A is "$NODE_A
echo "Node B is "$NODE_B
echo "Node C is "$NODE_C
echo $NODE_B > $HBASE_CONF_DIR/backup-masters
sed -i 's:NODE-A:'"$NODE_A"':g' $HBASE_CONF_DIR/hbase-site.xml
sed -i 's:NODE-B:'"$NODE_B"':g' $HBASE_CONF_DIR/hbase-site.xml
sed -i 's:NODE-C:'"$NODE_C"':g' $HBASE_CONF_DIR/hbase-site.xml
sed -i 's:MY_HBASE_SCRATCH:'"$SLURMTMPDIR"':g' $HBASE_CONF_DIR/hbase-site.xml
sed -i 's:MY_LOG_DIR:'"$MY_LOG_DIR"':g' $HBASE_CONF_DIR/hbase-env.sh
ls -l $HADOOP_CONF_DIR
echo "-------Start hdfs and yarn ---"
$HADOOP_HOME/sbin/start-all.sh 
#### Format HDFS, if this is the first time or not a persistent instance
echo "-------Show Report ---"
#$HADOOP_HOME/bin/hadoop dfsadmin -report

echo "-------start hbase--------"
$HBASE_HOME/bin/start-hbase.sh --config=$HBASE_CONF_DIR

echo "-------list directory ---"
#$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /
echo "-------make directory-----"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -mkdir /data
echo "-------copy files to hdfs-------"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -put ./data/*.csv /data/

echo "------- configure classpath for mapreduce job------"
for f in $HBASE_HOME/lib/*; do export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:$f; done

echo "------- run job -----------"
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar Uploader.jar /data 

echo "------- check hbase table --------"
$HBASE_HOME/bin/hbase shell $SLURM_SUBMIT_DIR/hbase-script 

echo "-------list directory ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /hbase

echo "--------stop hbase-------"
$HBASE_HOME/bin/stop-hbase.sh 
echo "-------Stop hdfs and yarn ---"
$HADOOP_HOME/sbin/stop-all.sh

#### Clean up the working directories after job completion
$MH_HOME/bin/myhadoop-cleanup.sh


